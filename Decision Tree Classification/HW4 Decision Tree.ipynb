{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad925468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tree_node:\n",
    "    \"\"\"\n",
    "    Data structure for nodes in the decision-tree\n",
    "    \"\"\"\n",
    "    def __init__(self,):\n",
    "        self.feature = None # index of the selected feature (for non-leaf node)\n",
    "        self.label = -1 # class label (for leaf node), -1 means the node is not a leaf node\n",
    "        self.left_child = None # left child node\n",
    "        self.right_child = None # right child node\n",
    "\n",
    "class Decision_tree:\n",
    "    \"\"\"\n",
    "    Decision tree with binary features\n",
    "    \"\"\"\n",
    "    def __init__(self,min_entropy):\n",
    "        self.min_entropy = min_entropy\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self,train_x,train_y):\n",
    "        # construct the decision-tree with recursion\n",
    "        self.root = self.generate_tree(train_x,train_y)\n",
    "\n",
    "    def predict(self,test_x):\n",
    "        # iterate through all samples\n",
    "        prediction = np.zeros([len(test_x),]).astype('int') # placeholder\n",
    "        for i in range(len(test_x)):\n",
    "            # traverse the decision-tree based on the features of the current sample till reaching a leaf node\n",
    "            pass # placeholder\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def generate_tree(self,data,label):\n",
    "        # initialize the current tree node\n",
    "        cur_node = Tree_node()\n",
    "\n",
    "        # compute the node entropy\n",
    "        node_entropy = self.compute_node_entropy(label)\n",
    "\n",
    "        # determine if the current node is a leaf node based on minimum node entropy (if yes, find the corresponding class label with majority voting and exit the current recursion)\n",
    "        if node_entropy < self.min_entropy:\n",
    "            cur_node.label = np.bincount(label).argmax()\n",
    "            return cur_node\n",
    "\n",
    "        # select the feature that will best split the current non-leaf node\n",
    "        selected_feature = self.select_feature(data,label)\n",
    "        cur_node.feature = selected_feature\n",
    "\n",
    "        # split the data based on the selected feature and start the next level of recursion\n",
    "        idx_0 = [data.T[selected_feature] == 0]\n",
    "        idx_1 = [data.T[selected_feature] == 1]\n",
    "        cur_node.left_child = self.generate_tree(data[tuple(idx_0)], label[tuple(idx_0)])\n",
    "        cur_node.right_child = self.generate_tree(data[tuple(idx_1)], label[tuple(idx_1)])\n",
    "\n",
    "        return cur_node\n",
    "\n",
    "    def select_feature(self,data,label):\n",
    "        # iterate through all features and compute their corresponding entropy\n",
    "        best_feat = 0\n",
    "        t = 1e10  # Just a large number\n",
    "        for i in range(len(data[0])):\n",
    "            # compute the entropy of splitting based on the selected features\n",
    "            tmp = data.T[i]\n",
    "            tmp_entropy = self.compute_split_entropy(\n",
    "                label[np.where(tmp == 0)], label[np.where(tmp == 1)]\n",
    "            )\n",
    "\n",
    "            # select the feature with minimum entropy\n",
    "            if t > tmp_entropy:\n",
    "                best_feat = i\n",
    "                t = tmp_entropy\n",
    "\n",
    "        return best_feat\n",
    "\n",
    "    def compute_split_entropy(self,left_y,right_y):\n",
    "        # compute the entropy of a potential split (with compute_node_entropy function), left_y and right_y are labels for the two branches\n",
    "        total = len(left_y) + len(right_y)\n",
    "        split_entropy = self.compute_node_entropy(left_y) * (\n",
    "            len(left_y) / total\n",
    "        ) + self.compute_node_entropy(right_y) * (len(right_y) / total)\n",
    "\n",
    "        return split_entropy\n",
    "\n",
    "    def compute_node_entropy(self,label):\n",
    "        # compute the entropy of a tree node (add 1e-15 inside the log2 when computing the entropy to prevent numerical issue)\n",
    "        _, counts = np.unique(label, return_counts=True)\n",
    "        node_entropy = sum(\n",
    "            [-(i / len(label)) * np.log2(i / len(label) + 1e-15) for i in counts]\n",
    "        )\n",
    "\n",
    "        return node_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e738a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "# from MyDecisionTree import Decision_tree\n",
    "\n",
    "# read in data.\n",
    "# training data\n",
    "train_data = np.genfromtxt(\"optdigits_train.txt\",delimiter=\",\")\n",
    "train_x = train_data[:,:-1]\n",
    "train_y = train_data[:,-1].astype('int')\n",
    "\n",
    "# validation data\n",
    "valid_data = np.genfromtxt(\"optdigits_valid.txt\",delimiter=\",\")\n",
    "valid_x = valid_data[:,:-1]\n",
    "valid_y = valid_data[:,-1].astype('int')\n",
    "\n",
    "# test data\n",
    "test_data = np.genfromtxt(\"optdigits_test.txt\",delimiter=\",\")\n",
    "test_x = test_data[:,:-1]\n",
    "test_y = test_data[:,-1].astype('int')\n",
    "\n",
    "############### Problem a ###################\n",
    "# experiment with different settings of minimum node entropy\n",
    "candidate_min_entropy = [0.01,0.05,0.1,0.2,0.5,0.8,1,2.0]\n",
    "valid_accuracy = []\n",
    "for i, min_entropy in enumerate(candidate_min_entropy):\n",
    "    # initialize the model\n",
    "    clf = Decision_tree(min_entropy=min_entropy)\n",
    "    # update the model based on training data, and record the best validation accuracy\n",
    "    clf.fit(train_x,train_y)\n",
    "    predictions_train = clf.predict(train_x)\n",
    "    predictions_val = clf.predict(valid_x)\n",
    "    cur_train_accuracy = np.count_nonzero(predictions_train.reshape(-1)==train_y.reshape(-1))/len(train_x)\n",
    "    cur_valid_accuracy = np.count_nonzero(predictions_val.reshape(-1)==valid_y.reshape(-1))/len(valid_x)\n",
    "    valid_accuracy.append(cur_valid_accuracy)\n",
    "    print('Training/validation accuracy for minimum node entropy %f is %.3f / %.3f' %(candidate_min_entropy[i],cur_train_accuracy,cur_valid_accuracy))\n",
    "\n",
    "# select the best minimum node entropy and use it to train the model\n",
    "best_entropy = candidate_min_entropy[np.argmax(valid_accuracy)]\n",
    "clf = Decision_tree(min_entropy=best_entropy)\n",
    "clf.fit(train_x,train_y)\n",
    "\n",
    "# evaluate on test data\n",
    "predictions = clf.predict(test_x)\n",
    "accuracy = np.count_nonzero(predictions.reshape(-1)==test_y.reshape(-1))/len(test_x)\n",
    "\n",
    "print('Test accuracy with minimum node entropy %f is %.3f' %(best_entropy,accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
